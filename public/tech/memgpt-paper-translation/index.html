<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MemGPT 论文中文翻译：将 LLM 作为操作系统 | Robert | 程序员 · 生活家</title>
<meta name=keywords content="MemGPT,LLM,AI,论文翻译,Agent"><meta name=description content="MemGPT 论文完整中文翻译，介绍如何通过虚拟上下文管理技术突破 LLM 的上下文窗口限制"><meta name=author content="Robert"><link rel=canonical href=https://robert-xblog.vercel.app/tech/memgpt-paper-translation/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://robert-xblog.vercel.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://robert-xblog.vercel.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://robert-xblog.vercel.app/favicon-32x32.png><link rel=apple-touch-icon href=https://robert-xblog.vercel.app/apple-touch-icon.png><link rel=mask-icon href=https://robert-xblog.vercel.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://robert-xblog.vercel.app/tech/memgpt-paper-translation/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="MemGPT 论文中文翻译：将 LLM 作为操作系统"><meta property="og:description" content="MemGPT 论文完整中文翻译，介绍如何通过虚拟上下文管理技术突破 LLM 的上下文窗口限制"><meta property="og:type" content="article"><meta property="og:url" content="https://robert-xblog.vercel.app/tech/memgpt-paper-translation/"><meta property="article:section" content="tech"><meta property="article:published_time" content="2026-02-22T21:30:00+08:00"><meta property="article:modified_time" content="2026-02-22T21:30:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MemGPT 论文中文翻译：将 LLM 作为操作系统"><meta name=twitter:description content="MemGPT 论文完整中文翻译，介绍如何通过虚拟上下文管理技术突破 LLM 的上下文窗口限制"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"\"MemGPT 论文中文翻译：将 LLM 作为操作系统\"","name":"\"MemGPT 论文中文翻译：将 LLM 作为操作系统\"","description":"\"MemGPT 论文完整中文翻译，介绍如何通过虚拟上下文管理技术突破 LLM 的上下文窗口限制\"","datePublished":"\"2026-02-22T21:30:00+08:00\"","dateModified":"\"2026-02-22T21:30:00+08:00\"","author":{"@type":"Person","name":"\"Robert\""},"mainEntityOfPage":{"@type":"WebPage","@id":"\"https://robert-xblog.vercel.app/tech/memgpt-paper-translation/\""}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://robert-xblog.vercel.app/ accesskey=h title="Robert | 程序员 · 生活家 (Alt + H)">Robert | 程序员 · 生活家</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://robert-xblog.vercel.app/tech/ title="💻 专业"><span>💻 专业</span></a></li><li><a href=https://robert-xblog.vercel.app/life/ title="🍃 人生"><span>🍃 人生</span></a></li><li><a href=https://robert-xblog.vercel.app/music/ title="🎵 兴趣"><span>🎵 兴趣</span></a></li><li><a href=https://robert-xblog.vercel.app/literature/ title="📜 文学"><span>📜 文学</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://robert-xblog.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://robert-xblog.vercel.app/tech/>💻 专业</a></div><h1 class=post-title>MemGPT 论文中文翻译：将 LLM 作为操作系统</h1><div class=post-description>MemGPT 论文完整中文翻译，介绍如何通过虚拟上下文管理技术突破 LLM 的上下文窗口限制</div><div class=post-meta><span title='2026-02-22 21:30:00 +0800 CST'>February 22, 2026</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;583 words&nbsp;·&nbsp;Robert</div></header><div class=post-content><blockquote><p><strong>原文标题</strong>: MemGPT: Towards LLMs as Operating Systems
<strong>作者</strong>: Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez
<strong>机构</strong>: 加州大学伯克利分校
<strong>arXiv</strong>: 2310.08560v2 [cs.AI] 2024年2月12日
<strong>翻译整理</strong>: 2025年2月</p></blockquote><hr><h2 id=摘要>摘要<a hidden class=anchor aria-hidden=true href=#摘要>#</a></h2><p>大语言模型（LLM）已经彻底改变了人工智能领域，但受到有限的上下文窗口限制，这阻碍了它们在扩展对话和文档分析等任务中的实用性。为了能够在有限的上下文窗口之外使用上下文，我们提出了<strong>虚拟上下文管理</strong>技术，这一技术借鉴了传统操作系统中的分层内存系统，通过物理内存和磁盘之间的分页来提供扩展虚拟内存的幻觉。</p><p>利用这一技术，我们引入了 <strong>MemGPT（MemoryGPT）</strong>，这是一个能够智能管理不同存储层级的系统，以在 LLM 有限的上下文窗口内有效提供扩展上下文。我们在两个领域评估了受操作系统启发的设计，在这些领域中，现代 LLM 的有限上下文窗口严重限制了它们的性能：</p><ol><li><strong>文档分析</strong>：MemGPT 能够分析远超底层 LLM 上下文窗口的大型文档</li><li><strong>多会话聊天</strong>：MemGPT 可以创建能够记住、反思并通过与用户的长期互动动态进化的对话智能体</li></ol><p>我们在 <a href=https://research.memgpt.ai>https://research.memgpt.ai</a> 发布了 MemGPT 代码和实验数据。</p><hr><p><img loading=lazy src=/images/memgpt/figure_1.jpg alt="图 1. MemGPT（左）在收到关于有限上下文空间的系统警报后将数据写入持久内存。">
<em>图 1. MemGPT（左）在收到关于有限上下文空间的系统警报后将数据写入持久内存。</em></p><p><img loading=lazy src=/images/memgpt/figure_2.jpg alt="图 2. MemGPT（左）可以搜索上下文外数据，将相关信息带入当前上下文窗口。">
<em>图 2. MemGPT（左）可以搜索上下文外数据，将相关信息带入当前上下文窗口。</em></p><hr><h2 id=1-引言>1. 引言<a hidden class=anchor aria-hidden=true href=#1-引言>#</a></h2><p>近年来，大语言模型（LLM）及其底层的 Transformer 架构（Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020; Ouyang et al., 2022）已成为对话式人工智能的基石，并催生了广泛的消费者和企业应用。尽管取得了这些进展，LLM 使用的有限固定长度上下文窗口显著阻碍了它们对长对话或长文档推理的适用性。例如，最广泛使用的开源 LLM 只能支持几十轮来回消息或推理短文档，然后就会超过其最大输入长度（Touvron et al., 2023）。</p><p>直接扩展 Transformer 的上下文长度会导致计算时间和内存成本的二次方增长，这是由于 Transformer 架构的自注意力机制造成的，这使得新长上下文架构的设计成为一个紧迫的研究挑战（Dai et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020）。虽然开发更长的模型是一个活跃的研究领域（Dong et al., 2023），即使我们能够克服上下文扩展的计算挑战，最近的研究表明长上下文模型难以有效利用额外的上下文（Liu et al., 2023a）。</p><p>因此，考虑到训练最先进 LLM 所需的大量资源以及上下文扩展的收益递减，迫切需要替代技术来支持长上下文。在本文中，我们研究了如何在使用固定上下文模型的同时提供无限上下文的幻觉。我们的方法借鉴了虚拟内存分页的思想，该技术通过主内存和磁盘之间的数据分页，使应用程序能够处理远超可用内存的数据集。我们利用 LLM 智能体函数调用能力的最新进展（Schick et al., 2023; Liu et al., 2023b）来设计 MemGPT。</p><hr><h2 id=2-memgptmemorygpt>2. MemGPT（MemoryGPT）<a hidden class=anchor aria-hidden=true href=#2-memgptmemorygpt>#</a></h2><p><img loading=lazy src=/images/memgpt/figure_3.jpg alt="图 3. 在 MemGPT 中，固定上下文 LLM 处理器通过分层内存系统和函数进行增强，使其能够管理自己的内存。">
<em>图 3. 在 MemGPT 中，固定上下文 LLM 处理器通过分层内存系统和函数进行增强，使其能够管理自己的内存。LLM 的提示词令牌（输入）或主上下文由系统指令、工作上下文和 FIFO 队列组成。LLM 完成令牌（输出）被函数执行器解释为函数调用。MemGPT 使用函数在主上下文和外部上下文（归档和回忆存储数据库）之间移动数据。</em></p><p>MemGPT 的受操作系统启发的多级内存架构区分了两种主要内存类型：</p><ul><li><strong>主上下文</strong>（类似于主内存/物理内存/RAM）</li><li><strong>外部上下文</strong>（类似于磁盘内存/磁盘存储）</li></ul><p>主上下文由 LLM 提示词令牌组成&ndash;主上下文中的任何内容都被视为上下文内，可以在推理期间被 LLM 处理器访问。外部上下文指的是保持在 LLM 固定上下文窗口之外的任何信息。这种上下文外数据必须始终被显式移动到主上下文中，才能在推理期间传递给 LLM 处理器。</p><p>MemGPT 提供函数调用，使 LLM 处理器能够管理自己的内存，无需任何用户干预。内存层级、操作系统函数和基于事件的控制流的结合使用使 MemGPT 能够使用具有有限上下文窗口的 LLM 处理无界上下文。</p><h3 id=21-主上下文提示词令牌>2.1 主上下文（提示词令牌）<a hidden class=anchor aria-hidden=true href=#21-主上下文提示词令牌>#</a></h3><p>MemGPT 中的提示词令牌分为三个连续部分：</p><ol><li><strong>系统指令</strong>：只读（静态），包含有关 MemGPT 控制流、不同内存级别的预期用途以及如何使用 MemGPT 函数的信息</li><li><strong>工作上下文</strong>：固定大小的读/写非结构化文本块，仅通过 MemGPT 函数调用可写。在对话设置中，工作上下文用于存储关于用户和智能体所采用角色的关键事实、偏好和其他重要信息，使智能体能够与用户流利地对话</li><li><strong>FIFO 队列</strong>：存储消息的滚动历史，包括智能体和用户之间的消息，以及系统消息和函数调用输入和输出。FIFO 队列中的第一个索引存储包含已从队列中逐出的消息的递归摘要的系统消息</li></ol><h3 id=22-队列管理器>2.2 队列管理器<a hidden class=anchor aria-hidden=true href=#22-队列管理器>#</a></h3><p>队列管理器管理回忆存储和 FIFO 队列中的消息。当系统收到新消息时，队列管理器将传入消息附加到 FIFO 队列，连接提示词令牌并触发 LLM 推理以生成 LLM 输出。队列管理器将传入消息和生成的 LLM 输出都写入回忆存储（MemGPT 消息数据库）。</p><p>当通过 MemGPT 函数调用检索回忆存储中的消息时，队列管理器将它们附加到队列的后面，以将它们重新插入 LLM 的上下文窗口。队列管理器还负责通过队列逐出策略控制上下文溢出。当提示词令牌超过底层 LLM 上下文窗口的"警告令牌计数"（例如上下文窗口的 70%）时，队列管理器在队列中插入系统消息，警告 LLM 即将发生的队列逐出（&ldquo;内存压力"警告），以允许 LLM 使用 MemGPT 函数将 FIFO 队列中的重要信息存储到工作上下文或归档存储。</p><hr><h2 id=3-实验>3. 实验<a hidden class=anchor aria-hidden=true href=#3-实验>#</a></h2><p>我们在两个长上下文领域评估 MemGPT：<strong>对话智能体</strong>和<strong>文档分析</strong>。对于对话智能体，我们扩展了现有的多会话聊天数据集（Xu et al., 2021），并引入了两个新的对话任务，评估智能体在扩展对话中保留知识的能力。对于文档分析，我们在 Liu 等人（2023a）的现有任务上对 MemGPT 进行基准测试，用于对冗长文档进行问答和键值检索。</p><p><img loading=lazy src=/images/memgpt/figure_4.jpg alt="图 4. 一个对话片段示例，MemGPT（左）更新存储的信息。这里信息存储在工作上下文内存中（位于提示词令牌内）。">
<em>图 4. 一个对话片段示例，MemGPT（左）更新存储的信息。这里信息存储在工作上下文内存中（位于提示词令牌内）。</em></p><h3 id=31-memgpt-用于对话智能体>3.1 MemGPT 用于对话智能体<a hidden class=anchor aria-hidden=true href=#31-memgpt-用于对话智能体>#</a></h3><p>对话智能体（如虚拟伴侣和个性化助手）旨在与用户进行自然的长期互动，可能持续数周、数月甚至数年。这为具有固定长度上下文的模型创造了挑战，这些模型只能引用有限的对话历史。&ldquo;无限上下文"智能体应无缝处理连续交流，没有边界或重置。</p><h4 id=311-深度记忆检索任务一致性>3.1.1 深度记忆检索任务（一致性）<a hidden class=anchor aria-hidden=true href=#311-深度记忆检索任务一致性>#</a></h4><p>我们基于 MSC 数据集引入了一个新的"深度记忆检索&rdquo;（DMR）任务，旨在测试对话智能体的一致性。在 DMR 中，用户向对话智能体提出一个明确引用先前对话的问题，并具有非常狭窄的预期答案范围。我们使用 LLM 生成 DMR 问答对，并用 ROUGE-L 分数和"LLM 评判"来评估生成响应的质量。</p><p><strong>表 1：深度记忆检索（DMR）性能</strong></p><table><thead><tr><th>模型</th><th>准确率 ⇑</th><th>ROUGE-L (R) ⇑</th></tr></thead><tbody><tr><td>GPT-3.5 Turbo</td><td>38.7%</td><td>0.394</td></tr><tr><td>+ MemGPT</td><td>66.9%</td><td>0.629</td></tr><tr><td>GPT-4</td><td>32.1%</td><td>0.296</td></tr><tr><td>+ MemGPT</td><td>92.5%</td><td>0.814</td></tr><tr><td>GPT-4 Turbo</td><td>35.3%</td><td>0.359</td></tr><tr><td>+ MemGPT</td><td>93.4%</td><td>0.827</td></tr></tbody></table><h4 id=312-对话开场白任务参与度>3.1.2 对话开场白任务（参与度）<a hidden class=anchor aria-hidden=true href=#312-对话开场白任务参与度>#</a></h4><p>我们评估智能体利用先前对话中积累的知识制作吸引人的消息的能力。为了评估对话开场白的"参与度&rdquo;，我们将生成的开场白与 gold personas 进行比较。</p><p><strong>表 2：对话开场白性能</strong></p><table><thead><tr><th>方法</th><th>SIM-1</th><th>SIM-3</th><th>SIM-H</th></tr></thead><tbody><tr><td>人工</td><td>0.800</td><td>0.800</td><td>1.000</td></tr><tr><td>GPT-3.5 Turbo</td><td>0.830</td><td>0.812</td><td>0.817</td></tr><tr><td>GPT-4</td><td>0.868</td><td>0.843</td><td>0.773</td></tr><tr><td>GPT-4 Turbo</td><td>0.857</td><td>0.828</td><td>0.767</td></tr></tbody></table><p>MemGPT 能够制作与人工编写的开场白相当甚至超过的吸引人的开场白。我们观察到 MemGPT 倾向于制作比人工基线更冗长且涵盖更多角色信息方面的开场白。</p><h3 id=32-memgpt-用于文档分析>3.2 MemGPT 用于文档分析<a hidden class=anchor aria-hidden=true href=#32-memgpt-用于文档分析>#</a></h3><p><img loading=lazy src=/images/memgpt/figure_5.jpg alt="图 5. 文档 QA 任务性能。MemGPT 的性能不受上下文长度增加的影响。使用 GPT-4 和 GPT-4 Turbo 运行 MemGPT 在此任务上具有等效的结果。">
<em>图 5. 文档 QA 任务性能。MemGPT 的性能不受上下文长度增加的影响。使用 GPT-4 和 GPT-4 Turbo 运行 MemGPT 在此任务上具有等效的结果。</em></p><p>文档分析也面临着当今 Transformer 模型有限上下文窗口的挑战。如表 3 所示，开源和闭源模型都受到上下文长度的限制（OpenAI 的模型最多 128k 令牌）。然而许多文档轻松超过这些长度；例如，法律或财务文件（如年度报告）可能轻松超过百万令牌。</p><p><strong>表 3：常用模型和 LLM API 的上下文长度比较</strong></p><table><thead><tr><th>模型 / API</th><th>开放?</th><th>上下文窗口（令牌）</th><th>大约消息数*</th></tr></thead><tbody><tr><td>Llama (1)</td><td>✓</td><td>2k</td><td>20</td></tr><tr><td>Llama 2</td><td>✓</td><td>4k</td><td>60</td></tr><tr><td>GPT-3.5 Turbo</td><td>✗</td><td>16k</td><td>300</td></tr><tr><td>GPT-4</td><td>✗</td><td>32k</td><td>~600</td></tr><tr><td>Claude 2</td><td>✗</td><td>100k</td><td>~2600</td></tr><tr><td>GPT-4 Turbo</td><td>✗</td><td>128k</td><td>~4000</td></tr></tbody></table><p>*假设预提示为 1k 令牌，平均消息大小为 ~50 令牌（~250 个字符）</p><h4 id=321-多文档问答>3.2.1 多文档问答<a hidden class=anchor aria-hidden=true href=#321-多文档问答>#</a></h4><p><img loading=lazy src=/images/memgpt/figure_6.jpg alt="图 6. MemGPT（左）解决文档 QA 任务的示例。维基百科文档数据库上传到归档存储。MemGPT 通过函数调用查询归档存储，将分页搜索结果拉入主上下文。">
<em>图 6. MemGPT（左）解决文档 QA 任务的示例。维基百科文档数据库上传到归档存储。MemGPT 通过函数调用查询归档存储，将分页搜索结果拉入主上下文。</em></p><p>为了评估 MemGPT 分析文档的能力，我们在来自 Liu 等人的检索器-阅读器文档 QA 任务上对 MemGPT 进行基准测试。MemGPT 的性能不受上下文长度增加的影响。虽然截断等方法可以扩展固定长度模型（如 GPT-4）的有效上下文长度，但随着所需压缩的增长，这种压缩方法将导致性能下降。</p><h4 id=322-嵌套键值检索kv>3.2.2 嵌套键值检索（KV）<a hidden class=anchor aria-hidden=true href=#322-嵌套键值检索kv>#</a></h4><p>我们引入了一个基于先前工作中提出的合成键值检索的新任务。在这个任务的嵌套版本中，值本身可能是键，因此需要智能体执行多跳查找。</p><p><img loading=lazy src=/images/memgpt/figure_7.jpg alt="图 7. 嵌套 KV 检索任务性能。MemGPT 是唯一能够在超过 2 级嵌套的情况下始终如一地完成嵌套 KV 任务的方法。">
<em>图 7. 嵌套 KV 检索任务性能。MemGPT 是唯一能够在超过 2 级嵌套的情况下始终如一地完成嵌套 KV 任务的方法。</em></p><p><img loading=lazy src=/images/memgpt/figure_8.jpg alt="图 8. MemGPT（左）解决嵌套 KV 任务的示例（为可读性缩短了 UUID）。在这个特定示例中，键值对有两个嵌套级别：831..ea5 → 5b8..4c3 → f37&mldr;617。当对最终值的查询只返回一个结果时，MemGPT 智能体返回最终答案，表明它也不是键。">
<em>图 8. MemGPT（左）解决嵌套 KV 任务的示例（为可读性缩短了 UUID）。在这个特定示例中，键值对有两个嵌套级别：831..ea5 → 5b8..4c3 → f37&mldr;617。当对最终值的查询只返回一个结果时，MemGPT 智能体返回最终答案，表明它也不是键。</em></p><p><strong>结果：</strong></p><ul><li>GPT-3.5 在 1 级嵌套时准确率降至 0%</li><li>GPT-4 和 GPT-4 Turbo 在 3 级嵌套时准确率降至 0%</li><li><strong>MemGPT 与 GPT-4 不受嵌套级别数量的影响</strong>，能够通过函数查询重复访问存储在主上下文中的键值对来执行嵌套查找</li></ul><hr><h2 id=4-相关工作>4. 相关工作<a hidden class=anchor aria-hidden=true href=#4-相关工作>#</a></h2><p>**长上下文 LLM：**几条工作线改进了 LLM 的上下文长度。例如，通过稀疏化注意力（Child et al., 2019; Beltagy et al., 2020）、低秩近似（Wang et al., 2020）和神经内存（Lee et al., 2019）实现更高效的 Transformer 架构。MemGPT 建立在这些上下文长度改进的基础上，因为它们提高了 MemGPT 中主内存的大小。</p><p>**检索增强模型：**MemGPT 的外部内存设计借鉴了大量使用外部检索器增强 LLM 的相关工作（Ram et al., 2023; Borgeaud et al., 2022; Karpukhin et al., 2020; Lewis et al., 2020; Guu et al., 2020）。特别是，Jiang 等人（2023）提出了 FLARE，一种允许 LLM 在生成过程中主动决定何时以及检索什么的方法。</p><p>**作为智能体的 LLM：**最近的工作探索了增强 LLM 的能力，使其能够在交互式环境中充当智能体。Park 等人（2023）提出向 LLM 添加内存并使用 LLM 作为规划器。与我们的工作相反，这些工作侧重于为智能体配备用户输入的长期记忆。</p><hr><h2 id=5-结论>5. 结论<a hidden class=anchor aria-hidden=true href=#5-结论>#</a></h2><p>在本文中，我们介绍了 MemGPT，一种受操作系统启发的新型 LLM 系统，用于管理大语言模型的有限上下文窗口。通过设计类似于传统操作系统的内存层级和控制流，MemGPT 为 LLM 提供了更大上下文资源的幻觉。这种受操作系统启发的方法在两个领域进行了评估，其中现有 LLM 的性能受到有限上下文长度的限制：文档分析和对话智能体。</p><p>对于文档分析，MemGPT 能够通过有效地将相关上下文分页进出内存来处理远超当前 LLM 上下文限制的长文本。对于对话智能体，MemGPT 能够在扩展对话中保持长期记忆、一致性和可进化性。总的来说，MemGPT 证明了操作系统技术（如分层内存管理和中断）即使在受固定上下文长度限制时也能释放 LLM 的潜力。</p><p>这项工作为未来的探索开辟了众多途径，包括将 MemGPT 应用于其他具有大量或无界上下文的领域，集成不同的内存层技术（如数据库或缓存），以及进一步改进控制流和内存管理策略。通过将操作系统架构的概念引入 AI 系统，MemGPT 代表了在其基本限制内最大化 LLM 能力的有希望的新方向。</p><h3 id=核心贡献总结>核心贡献总结<a hidden class=anchor aria-hidden=true href=#核心贡献总结>#</a></h3><ol><li>**虚拟上下文管理：**首次将 OS 虚拟内存思想应用于 LLM，实现无限上下文的幻觉</li><li>**分层存储管理：**主上下文 ↔ 外部存储的自动交换，类似于 CPU 缓存层次结构</li><li>**中断驱动控制流：**函数调用作为"系统中断"，让 Agent 主动管理内存</li><li>**生产就绪系统：**完整的 API、SDK 和 CLI 工具链</li></ol><h3 id=引用信息>引用信息<a hidden class=anchor aria-hidden=true href=#引用信息>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>packer2023memgpt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span><span class=p>=</span><span class=s>{{MemGPT}: Towards LLMs as Operating Systems}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span><span class=p>=</span><span class=s>{Packer, Charles and Wooders, Sarah and Lin, Kevin and
</span></span></span><span class=line><span class=cl><span class=s>          Fang, Vivian and Patil, Shishir G. and Stoica, Ion and
</span></span></span><span class=line><span class=cl><span class=s>          Gonzalez, Joseph E.}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span><span class=p>=</span><span class=s>{arXiv preprint arXiv:2310.08560}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span><span class=p>=</span><span class=s>{2023}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=相关资源>相关资源<a hidden class=anchor aria-hidden=true href=#相关资源>#</a></h3><ul><li>官网: <a href=https://letta.ai>https://letta.ai</a></li><li>文档: <a href=https://docs.letta.com>https://docs.letta.com</a></li><li>GitHub: <a href=https://github.com/letta-ai/letta>https://github.com/letta-ai/letta</a></li><li>论文: <a href=https://research.memgpt.ai>https://research.memgpt.ai</a></li><li>Discord: <a href=https://discord.gg/letta>https://discord.gg/letta</a></li></ul><hr><p><em>中文翻译整理完成于 2025年2月</em>
<em>原文: arXiv:2310.08560 [cs.AI]</em>
<em>翻译说明：本翻译保留了原文的所有图表、表格结构和关键术语，同时提供了完整的中文解释</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://robert-xblog.vercel.app/tags/memgpt/>MemGPT</a></li><li><a href=https://robert-xblog.vercel.app/tags/llm/>LLM</a></li><li><a href=https://robert-xblog.vercel.app/tags/ai/>AI</a></li><li><a href=https://robert-xblog.vercel.app/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/>论文翻译</a></li><li><a href=https://robert-xblog.vercel.app/tags/agent/>Agent</a></li></ul><nav class=paginav><a class=next href=https://robert-xblog.vercel.app/tech/memgpt-letta-guide/><span class=title>Next »</span><br><span>MemGPT/Letta 记忆与上下文管理深度解析</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://robert-xblog.vercel.app/>Robert | 程序员 · 生活家</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>